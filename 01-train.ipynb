{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdb85b2a4d141b08a5144476814bcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e406c9f97554822b290ee883356d9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7649e88d16ea43e39f926f7387460802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d9a3dbe404a7d97c539b3eb927620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root='mnist',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model structure\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 2),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        codes = self.encoder(inputs)\n",
    "        decoded = self.decoder(codes)\n",
    "\n",
    "        return codes, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss: 0.04460776969790459\n",
      "[2/10] Loss: 0.044061582535505295\n",
      "[3/10] Loss: 0.03978953883051872\n",
      "[4/10] Loss: 0.042708661407232285\n",
      "[5/10] Loss: 0.042434729635715485\n",
      "[6/10] Loss: 0.04136792570352554\n",
      "[7/10] Loss: 0.04267749562859535\n",
      "[8/10] Loss: 0.0400136262178421\n",
      "[9/10] Loss: 0.04271965101361275\n",
      "[10/10] Loss: 0.041508741676807404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/jupyterlab/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type AutoEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "model = AutoEncoder()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Train\n",
    "for epoch in range(epochs):\n",
    "    for data, labels in train_loader:\n",
    "        inputs = data.view(-1, 784)\n",
    "\n",
    "        # Forward\n",
    "        codes, decoded = model(inputs)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(decoded, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Show progress\n",
    "    print('[{}/{}] Loss:'.format(epoch+1, epochs), loss.item())\n",
    "\n",
    "\n",
    "# Save\n",
    "torch.save(model, 'autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_segmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c60121cc72dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_segmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvgg_unet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m51\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0minput_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m608\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.train(\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_segmentation'"
     ]
    }
   ],
   "source": [
    "from keras_segmentation.models.unet import vgg_unet\n",
    "\n",
    "model = vgg_unet(n_classes=51 ,  input_height=416, input_width=608  )\n",
    "\n",
    "model.train(\n",
    "    train_images =  \"dataset1/images_prepped_train/\",\n",
    "    train_annotations = \"dataset1/annotations_prepped_train/\",\n",
    "    checkpoints_path = \"/tmp/vgg_unet_1\" , epochs=5\n",
    ")\n",
    "\n",
    "out = model.predict_segmentation(\n",
    "    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n",
    "    out_fname=\"/tmp/out.png\"\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(out)\n",
    "\n",
    "# evaluating the model \n",
    "print(model.evaluate_segmentation( inp_images_dir=\"dataset1/images_prepped_test/\"  , annotations_dir=\"dataset1/annotations_prepped_test/\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_segmentation\n",
      "  Downloading https://files.pythonhosted.org/packages/43/f0/b8def71a219c6a21f5201727082e846c560817712b3484e8f0c834c9c0e6/keras_segmentation-0.3.0.tar.gz\n",
      "Requirement already satisfied: Keras>=2.0.0 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from keras_segmentation) (2.2.4)\n",
      "Requirement already satisfied: imageio==2.5.0 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from keras_segmentation) (2.5.0)\n",
      "Collecting imgaug==0.2.9 (from keras_segmentation)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/a9/36de8c0e1ffb2d86f871cac60e5caa910cbbdb5f4741df5ef856c47f4445/imgaug-0.2.9-py2.py3-none-any.whl (753kB)\n",
      "\u001b[K     |████████████████████████████████| 757kB 2.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from keras_segmentation) (4.1.0.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from keras_segmentation) (4.32.2)\n",
      "Requirement already satisfied: h5py in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from Keras>=2.0.0->keras_segmentation) (2.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from Keras>=2.0.0->keras_segmentation) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from Keras>=2.0.0->keras_segmentation) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from Keras>=2.0.0->keras_segmentation) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from Keras>=2.0.0->keras_segmentation) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from Keras>=2.0.0->keras_segmentation) (5.1.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from Keras>=2.0.0->keras_segmentation) (1.1.0)\n",
      "Requirement already satisfied: pillow in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from imageio==2.5.0->keras_segmentation) (6.1.0)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from imgaug==0.2.9->keras_segmentation) (0.15.0)\n",
      "Collecting Shapely (from imgaug==0.2.9->keras_segmentation)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/f8/db4d3426a1aba9d5dfcc83ed5a3e2935d2b1deb73d350642931791a61c37/Shapely-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 7.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from imgaug==0.2.9->keras_segmentation) (3.1.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug==0.2.9->keras_segmentation) (1.0.3)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug==0.2.9->keras_segmentation) (2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from matplotlib->imgaug==0.2.9->keras_segmentation) (0.10.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.9->keras_segmentation) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/jupyterlab/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug==0.2.9->keras_segmentation) (41.0.1)\n",
      "Building wheels for collected packages: keras-segmentation\n",
      "  Building wheel for keras-segmentation (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d0/bb/c0/6aad88b38f6e46db048bed4cccb904a5897055a8ab6fbd4dfc\n",
      "Successfully built keras-segmentation\n",
      "Installing collected packages: Shapely, imgaug, keras-segmentation\n",
      "Successfully installed Shapely-1.7.1 imgaug-0.2.9 keras-segmentation-0.3.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
